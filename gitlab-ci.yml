image: ubuntu

variables:
  SONAR_URL: ${SONAR_URL}
  SONAR_TOKEN: ${SONAR_TOKEN}
  PROJECT_KEYS: 
  PROJECT_NAME: 
  IMAGE_NAME: 
  MVP_ID: "AS"
  INSIDR_API_ENDPOINT: 
  INSIDR_USERNAME: sonarUser
  INSIDR_TOKEN: password
  KUBECONFIG: ${CI_PROJECT_DIR}/kube_config
  HELM_VERSION: "v2.11.0"
  KUBE_LATEST_VERSION: "v1.11.3"
  # Dast Variables
  MVP_APP_URL:
  DAST_DOCKER_IMAGE: 
  MVP_SHORTNAME: $MVP_ID
  AZURE_STORAGE_CONNECTION_STRING: ${SCANS_STORAGE_CONNECTION_STRING}
  AZURE_LOGIN_PWD: ${AZURE_USER_PWD}
  AZURE_LOGIN_USER: ${AZURE_USER_NAME}
  IP_THALES: 

stages:
  - test
  - sonar
  - sonar-quality-gate
  - TechnicalDebtToInsidr
  - build
  - deploy-to-sandbox
  - deploy-to-prod

test:
  stage: test
  image: node:lts-alpine
  script:
    - apk add --no-cache git
    - npm i -g yarn lerna
    - yarn
    - lerna bootstrap
    - yarn test:coverage
  cache:
    key: node-modules
    paths:
      - node_modules/
      - /usr/local/lib/node_modules/
  artifacts:
    name: reports-${CI_COMMIT_REF_NAME}
    when: on_success
    expire_in: 30 mins
    paths:
      - reports/

sonar:
  stage: sonar
  image: zaquestion/sonarqube-scanner
  script:
    - rm -rf /root/sonar_home/conf
    - >
      sonar-scanner
      -D project.settings=sonar.properties
      -D sonar.gitlab.commit_sha=${CI_COMMIT_SHA}
      -D sonar.gitlab.project_id=${CI_PROJECT_ID}
      -D sonar.gitlab.ref_name=${CI_COMMIT_REF_NAME}
      -D sonar.projectKey=${PROJECT_KEYS}
      -D sonar.projectName=${PROJECT_NAME}
      -D sonar.host.url=${SONAR_URL}
      -D sonar.login=${SONAR_TOKEN}
  only:
    - master

sonarqube-insidr-python:
  stage: TechnicalDebtToInsidr
  image: docker
  script:
    - docker login -u "$CI_REGISTRY_USER" -p "$CI_JOB_TOKEN" "$CI_REGISTRY"
    - >
      docker run --rm gitlab:5005/soarizon/devops/sonarqube-insidr/sonar_insidr:0.1
      --sonar-url "$SONAR_URL"
      --sonar-token "$SONAR_TOKEN"
      --projects-keys $PROJECT_KEYS
      --mvp-id "$MVP_ID"
      --insidr-api-endpoint "$INSIDR_API_ENDPOINT"
      --insidr-username "$INSIDR_USERNAME"
      --insidr-token "$INSIDR_TOKEN" || true
  only:
    - master

sonar-quality-gate:
 stage: sonar-quality-gate
 script:
   - cat /etc/os-release
   - apt-get update -y
   - apt-get install jq -y
   - apt-get install curl -y
   - RESULT=$(curl https://quality-analysis/sonar/api/qualitygates/project_status\?projectKey\=${PROJECT_KEYS} -s -u ${SONAR_TOKEN_AS_USER} | jq '. | .projectStatus.status')
   - echo "Quality Gate Status $RESULT"
   - |
     if [ $RESULT = "\"ERROR\"" ]; then
      echo "Quality gate failed. Pipeline stops";
      echo "Go to: ${SONAR_URL}/dashboard?id=${PROJECT_KEYS}";
      exit 1;
     else
      echo "Quality gate is ok. Pipeline continues";
      exit 0;
     fi
 only:
  - master

build-asteq-server:
  stage: build
  image: docker
  script:
    - docker login -u "$CI_REGISTRY_USER" -p "$CI_JOB_TOKEN" "$CI_REGISTRY"
    - docker build -f Dockerfile.server -t "$CI_REGISTRY_IMAGE/$IMAGE_NAME:$CI_COMMIT_SHA" .
    - docker push "$CI_REGISTRY_IMAGE/$IMAGE_NAME:$CI_COMMIT_SHA"
  only:
    - master

deploy-asteq-server:
  stage: deploy-to-sandbox
  image: dtzar/helm-kubectl:2.11.0
  before_script:
    - echo ${KUBE_SANDBOX_CONFIG} | base64 -d > ${KUBECONFIG}
    - export KUBECONFIG=${KUBECONFIG}
    - cd packages/server/deployment/
  script:
    - export KUBECONFIG=${KUBECONFIG}
    - echo " >>>>>>>>>> Deploy in progress for $CI_PROJECT_NAME <<<<<<<<<< "
    - >
      helm upgrade
      --install $IMAGE_NAME
      --set replicaCount="1"
      --set image.repository="$CI_REGISTRY_IMAGE/$IMAGE_NAME"
      --set image.tag="$CI_COMMIT_SHA"
      --set adFrontClientId="$SANDBOX_AD_FRONT_CLIENT_ID"
      --set adBackClientId="$SANDBOX_AD_BACK_CLIENT_ID"
      --set adTenantId="$SANDBOX_AD_TENANT_ID"
      --set azureAppInsightsIKey="$SANDBOX_AZURE_APP_INSIGHTS_IKEY"
      --set asteqEnv="sandbox"
      --set scanUser="$SANDBOX_SCAN_USER"
      --set scanPassword="$SANDBOX_SCAN_PASSWORD"
      --namespace $IMAGE_NAME
      --wait .
  environment:
    name: sandbox
  only:
    - master

deploy-asteq-server-prod:
  stage: deploy-to-prod
  image: dtzar/helm-kubectl:2.11.0
  before_script:
    - echo ${KUBE_PROD_CONFIG} | base64 -d > ${KUBECONFIG}
    - export KUBECONFIG=${KUBECONFIG}
    - cd packages/server/deployment/
  script:
    - export KUBECONFIG=${KUBECONFIG}
    - echo " >>>>>>>>>> Deploy in progress for $CI_PROJECT_NAME in production <<<<<<<<<< "
    - >
      helm upgrade
      --install $IMAGE_NAME
      --set replicaCount="3"
      --set image.repository="$CI_REGISTRY_IMAGE/$IMAGE_NAME"
      --set image.tag="$CI_COMMIT_SHA"
      --set adFrontClientId="$PRODUCTION_AD_FRONT_CLIENT_ID"
      --set adBackClientId="$PRODUCTION_AD_BACK_CLIENT_ID"
      --set adTenantId="$PRODUCTION_AD_TENANT_ID"
      --set azureAppInsightsIKey="$PRODUCTION_AZURE_APP_INSIGHTS_IKEY"
      --set ingress.hosts.rules=
      --set ingress.tls[0].hosts[0]=
      --set ingress.tls[0].secretName=
      --set ingress.annotations."nginx\.ingress\.kubernetes\.io/whitelist-source-range"="$IP_THALES"
      --set asteqEnv="production"
      --set scanUser="$PRODUCTION_SCAN_USER"
      --set scanPassword="$PRODUCTION_SCAN_PASSWORD"
      --namespace $IMAGE_NAME
      --wait .
  when: manual
  environment:
    name: prod
  only:
    - master
